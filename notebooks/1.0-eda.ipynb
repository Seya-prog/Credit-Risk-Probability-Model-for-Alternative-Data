{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Credit Risk Probability Model - Exploratory Data Analysis\n",
        "\n",
        "This notebook contains exploratory analysis of the eCommerce transaction data to develop insights for the credit risk model. The goal is to identify patterns, data quality issues, and form hypotheses to guide feature engineering.\n",
        "\n",
        "## Task Overview\n",
        "We need to develop a credit scoring model that utilizes eCommerce behavioral data to assess credit risk for a buy-now-pay-later service. This requires:\n",
        "\n",
        "1. Defining a proxy variable for credit risk categorization (high/low risk)\n",
        "2. Selecting predictive features that correlate with default risk\n",
        "3. Building models to assign risk probability and credit scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import datetime as dt\n",
        "\n",
        "# Set visualization options\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette('viridis')\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Loading and Exploring the Data Structure\n",
        "\n",
        "The data is available from the Xente Challenge on Kaggle. Here we will load the dataset and explore its structure to understand the available features and data types.\n",
        "\n",
        "**Note:** In a real-world scenario, you would need to download the data from the provided link and place it in the `data/raw` directory. For this notebook, we'll use a placeholder path that you should update with the actual location of your data file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "# Update this path to the location of your data file\n",
        "data_file = \"../data/raw/data.csv\"\n",
        "\n",
        "try:\n",
        "    # Try to load the data if the file exists\n",
        "    df = pd.read_csv(data_file)\n",
        "    print(f\"Data loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Data file not found at {data_file}\")\n",
        "    print(\"Please download the data from the Xente Challenge on Kaggle and place it in the data/raw directory\")\n",
        "    # Create a sample dataframe with the expected columns for demonstration\n",
        "    df = pd.DataFrame({\n",
        "        'TransactionId': ['T1001', 'T1002', 'T1003'],\n",
        "        'BatchId': [1, 1, 2],\n",
        "        'AccountId': ['A001', 'A002', 'A001'],\n",
        "        'SubscriptionId': ['S001', 'S002', 'S001'],\n",
        "        'CustomerId': ['C001', 'C002', 'C001'],\n",
        "        'CurrencyCode': ['UGX', 'UGX', 'UGX'],\n",
        "        'CountryCode': [123, 123, 123],\n",
        "        'ProviderId': ['P001', 'P002', 'P003'],\n",
        "        'ProductId': ['PROD001', 'PROD002', 'PROD003'],\n",
        "        'ProductCategory': ['Airtime', 'Utility', 'Airtime'],\n",
        "        'ChannelId': ['web', 'android', 'web'],\n",
        "        'Amount': [5000, -2000, 10000],\n",
        "        'Value': [5000, 2000, 10000],\n",
        "        'TransactionStartTime': ['2018-10-01 10:10:10', '2018-10-01 11:10:10', '2018-10-02 10:10:10'],\n",
        "        'PricingStrategy': ['A', 'B', 'A'],\n",
        "        'FraudResult': [0, 1, 0]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the structure of the data\n",
        "print(\"Data types for each column:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset info:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Summary Statistics\n",
        "\n",
        "Let's calculate summary statistics for the numerical features to understand their central tendencies, dispersion, and shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for numerical variables\n",
        "print(\"Summary statistics for numerical variables:\")\n",
        "df.describe().T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert transaction time to datetime\n",
        "if 'TransactionStartTime' in df.columns:\n",
        "    df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
        "    \n",
        "    # Extract time-based features\n",
        "    df['TransactionYear'] = df['TransactionStartTime'].dt.year\n",
        "    df['TransactionMonth'] = df['TransactionStartTime'].dt.month\n",
        "    df['TransactionDay'] = df['TransactionStartTime'].dt.day\n",
        "    df['TransactionDayOfWeek'] = df['TransactionStartTime'].dt.dayofweek\n",
        "    df['TransactionHour'] = df['TransactionStartTime'].dt.hour\n",
        "    \n",
        "    print(\"Added datetime features from TransactionStartTime\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Distribution of Numerical Features\n",
        "\n",
        "Let's visualize the distributions of key numerical features to identify patterns, skewness, and potential outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions of Amount and Value\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Amount distribution\n",
        "if 'Amount' in df.columns:\n",
        "    sns.histplot(df['Amount'], kde=True, ax=axes[0])\n",
        "    axes[0].set_title('Distribution of Amount')\n",
        "    axes[0].set_xlabel('Amount')\n",
        "    \n",
        "    # Value distribution\n",
        "    if 'Value' in df.columns:\n",
        "        sns.histplot(df['Value'], kde=True, ax=axes[1])\n",
        "        axes[1].set_title('Distribution of Value')\n",
        "        axes[1].set_xlabel('Value')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots for numerical features to identify outliers\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "if len(numerical_cols) > 0:\n",
        "    # Select a subset of numerical columns for boxplot\n",
        "    plot_cols = [col for col in numerical_cols if col not in ['CustomerId', 'BatchId']][:4]  # Limit to 4 columns\n",
        "    \n",
        "    if plot_cols:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        sns.boxplot(data=df[plot_cols])\n",
        "        plt.title('Boxplots of Numerical Features')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No numerical columns found for boxplot visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Distribution of Categorical Features\n",
        "\n",
        "Let's analyze the distribution of categorical features to understand their frequency and variability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove datetime columns from categorical columns if any\n",
        "if 'TransactionStartTime' in categorical_cols:\n",
        "    categorical_cols.remove('TransactionStartTime')\n",
        "\n",
        "# Print unique values for each categorical column\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    print(f\"Number of unique values: {df[col].nunique()}\")\n",
        "    print(\"Value counts:\")\n",
        "    display(df[col].value_counts().head(10))  # Display top 10 values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distribution of key categorical variables\n",
        "key_categorical = ['ProductCategory', 'ChannelId', 'PricingStrategy'] \n",
        "key_categorical = [col for col in key_categorical if col in df.columns]\n",
        "\n",
        "if key_categorical:\n",
        "    fig, axes = plt.subplots(len(key_categorical), 1, figsize=(12, 5*len(key_categorical)))\n",
        "    \n",
        "    # Handle case of one categorical variable\n",
        "    if len(key_categorical) == 1:\n",
        "        axes = [axes]\n",
        "        \n",
        "    for i, col in enumerate(key_categorical):\n",
        "        value_counts = df[col].value_counts()\n",
        "        # If there are too many categories, limit to top 10\n",
        "        if len(value_counts) > 10:\n",
        "            value_counts = value_counts.nlargest(10)\n",
        "            \n",
        "        sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {col}')\n",
        "        axes[i].set_ylabel('Count')\n",
        "        axes[i].set_xlabel(col)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Missing Values Analysis\n",
        "\n",
        "Let's check for missing values in the dataset and decide on appropriate imputation strategies if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "# Create a dataframe with the results\n",
        "missing_info = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Missing Percentage': missing_percentage\n",
        "})\n",
        "\n",
        "# Display only columns with missing values\n",
        "missing_info = missing_info[missing_info['Missing Values'] > 0].sort_values('Missing Percentage', ascending=False)\n",
        "\n",
        "if len(missing_info) > 0:\n",
        "    print(\"Columns with missing values:\")\n",
        "    display(missing_info)\n",
        "else:\n",
        "    print(\"No missing values found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Correlation Analysis\n",
        "\n",
        "Let's examine the correlation between numerical features to identify potential relationships and dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numerical features for correlation analysis\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Ensure there are enough numerical variables for meaningful correlation\n",
        "if len(numerical_cols) >= 2:\n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = df[numerical_cols].corr()\n",
        "    \n",
        "    # Plot correlation heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title('Correlation Heatmap of Numerical Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Identify highly correlated pairs (|corr| > 0.7)\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
        "                high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], \n",
        "                                      correlation_matrix.iloc[i, j]))\n",
        "                \n",
        "    if high_corr_pairs:\n",
        "        print(\"Highly correlated feature pairs (|corr| > 0.7):\")\n",
        "        for col1, col2, corr in high_corr_pairs:\n",
        "            print(f\"{col1} - {col2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"No highly correlated feature pairs found.\")\n",
        "else:\n",
        "    print(\"Not enough numerical columns for correlation analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. FraudResult Analysis\n",
        "\n",
        "Since FraudResult could be a potential proxy for credit risk, let's analyze its distribution and relationship with other variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if FraudResult is available in the dataset\n",
        "if 'FraudResult' in df.columns:\n",
        "    # Distribution of FraudResult\n",
        "    print(\"Distribution of FraudResult:\")\n",
        "    fraud_counts = df['FraudResult'].value_counts()\n",
        "    print(fraud_counts)\n",
        "    \n",
        "    # Calculate percentage of each class\n",
        "    fraud_percentage = (fraud_counts / len(df)) * 100\n",
        "    print(\"\\nPercentage distribution:\")\n",
        "    for label, percentage in fraud_percentage.items():\n",
        "        print(f\"FraudResult = {label}: {percentage:.2f}%\")\n",
        "    \n",
        "    # Visualize FraudResult distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.countplot(x='FraudResult', data=df)\n",
        "    plt.title('Distribution of FraudResult')\n",
        "    \n",
        "    # Add count labels on top of each bar\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{int(p.get_height())}', \n",
        "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                   ha = 'center', va = 'bottom')\n",
        "    \n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"FraudResult column not found in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze FraudResult by key features\n",
        "if 'FraudResult' in df.columns:\n",
        "    # Key features to analyze against FraudResult\n",
        "    key_features = ['ProductCategory', 'ChannelId', 'PricingStrategy']\n",
        "    key_features = [col for col in key_features if col in df.columns]\n",
        "    \n",
        "    if key_features:\n",
        "        fig, axes = plt.subplots(len(key_features), 1, figsize=(12, 6*len(key_features)))\n",
        "        \n",
        "        # Handle case of one feature\n",
        "        if len(key_features) == 1:\n",
        "            axes = [axes]\n",
        "            \n",
        "        for i, feature in enumerate(key_features):\n",
        "            fraud_by_feature = pd.crosstab(df[feature], df['FraudResult'], normalize='index') * 100\n",
        "            fraud_by_feature.plot(kind='bar', ax=axes[i], stacked=False)\n",
        "            axes[i].set_title(f'Fraud Rate by {feature}')\n",
        "            axes[i].set_ylabel('Percentage (%)')\n",
        "            axes[i].set_xlabel(feature)\n",
        "            axes[i].legend(title='FraudResult')\n",
        "            \n",
        "            # Add percentage labels\n",
        "            for p in axes[i].patches:\n",
        "                width = p.get_width()\n",
        "                height = p.get_height()\n",
        "                x, y = p.get_xy() \n",
        "                if height > 5:  # Only add label if segment is large enough\n",
        "                    axes[i].annotate(f'{height:.1f}%', (x + width/2, y + height/2), \n",
        "                                   ha='center', va='center')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"FraudResult column not found in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. RFM Analysis\n",
        "\n",
        "Creating RFM (Recency, Frequency, Monetary) metrics will be crucial for our credit risk model. Let's compute these metrics and analyze their distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create RFM metrics if necessary data is available\n",
        "if 'TransactionStartTime' in df.columns and 'CustomerId' in df.columns and ('Amount' in df.columns or 'Value' in df.columns):\n",
        "    # Use the most recent date as a reference point\n",
        "    if len(df) > 0:\n",
        "        snapshot_date = df['TransactionStartTime'].max() + pd.Timedelta(days=1)\n",
        "        \n",
        "        # Monetary value - use Value instead of Amount as Amount can be negative\n",
        "        monetary_col = 'Value' if 'Value' in df.columns else 'Amount'\n",
        "        \n",
        "        # Calculate RFM metrics per customer\n",
        "        rfm = df.groupby('CustomerId').agg({\n",
        "            'TransactionStartTime': lambda x: (snapshot_date - x.max()).days,  # Recency\n",
        "            'TransactionId': 'count',  # Frequency\n",
        "            monetary_col: 'sum'  # Monetary\n",
        "        }).rename(columns={\n",
        "            'TransactionStartTime': 'Recency',\n",
        "            'TransactionId': 'Frequency',\n",
        "            monetary_col: 'Monetary'\n",
        "        })\n",
        "        \n",
        "        # Display RFM metrics\n",
        "        print(\"RFM Metrics for Customers:\")\n",
        "        display(rfm.head())\n",
        "        \n",
        "        # Summary statistics for RFM metrics\n",
        "        print(\"\\nRFM Summary Statistics:\")\n",
        "        display(rfm.describe())\n",
        "        \n",
        "        # Visualize distributions\n",
        "        plt.figure(figsize=(18, 6))\n",
        "        \n",
        "        # Recency distribution\n",
        "        plt.subplot(1, 3, 1)\n",
        "        sns.histplot(rfm['Recency'], kde=True)\n",
        "        plt.title('Recency Distribution')\n",
        "        plt.xlabel('Recency (days)')\n",
        "        \n",
        "        # Frequency distribution \n",
        "        plt.subplot(1, 3, 2)\n",
        "        sns.histplot(rfm['Frequency'], kde=True)\n",
        "        plt.title('Frequency Distribution')\n",
        "        plt.xlabel('Frequency (count)')\n",
        "        \n",
        "        # Monetary distribution\n",
        "        plt.subplot(1, 3, 3)\n",
        "        sns.histplot(rfm['Monetary'], kde=True)\n",
        "        plt.title('Monetary Distribution')\n",
        "        plt.xlabel('Monetary Value')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Empty DataFrame, cannot calculate RFM metrics\")\n",
        "else:\n",
        "    print(\"Missing required columns for RFM analysis (TransactionStartTime, CustomerId, Amount/Value)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. Key Insights and Next Steps\n",
        "\n",
        "Based on our exploratory data analysis, here are the key insights and proposed next steps for feature engineering and modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of Key Insights\n",
        "\n",
        "\"\"\"\n",
        "Key Insights from EDA:\n",
        "\n",
        "1. **Proxy Variable Definition**: \n",
        "   - The FraudResult column provides a potential starting point for creating our credit risk proxy\n",
        "   - We may need to combine this with RFM metrics to create a more comprehensive risk proxy\n",
        "   - The class imbalance in FraudResult suggests we'll need appropriate handling techniques\n",
        "\n",
        "2. **Feature Importance**:\n",
        "   - Transactional features like Amount and Value show significant variation and potential predictive power\n",
        "   - Categorical variables (ProductCategory, ChannelId, PricingStrategy) show varying distributions of fraud\n",
        "   - Time-based patterns may provide strong signals for credit risk prediction  \n",
        "\n",
        "3. **RFM Analysis**:\n",
        "   - Customer behavior patterns captured through RFM metrics show distinct segments\n",
        "   - The monetary distribution indicates potential outliers that may need special handling\n",
        "   - Frequency patterns could be strong predictors of creditworthiness\n",
        "\n",
        "4. **Data Quality**:\n",
        "   - [Note any missing values or data quality issues identified]\n",
        "   - [Note any unusual distributions or outliers discovered]\n",
        "   - [Note any inconsistencies or anomalies in the data]\n",
        "\n",
        "5. **Relationships and Correlations**:\n",
        "   - Strong correlation between [highlight any strong correlations discovered]\n",
        "   - Potential multicollinearity between [mention any concerning correlations]\n",
        "   - Nonlinear relationships between [mention any interesting nonlinear patterns]\n",
        "\n",
        "Next Steps for Feature Engineering:\n",
        "1. Create comprehensive RFM features per customer\n",
        "2. Engineer temporal features (day of week patterns, seasonality, etc.)\n",
        "3. Develop behavioral segments based on transaction patterns\n",
        "4. Transform skewed numerical features\n",
        "5. Create a robust proxy variable for credit risk by combining FraudResult with RFM metrics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\"\"Note: This is a placeholder analysis based on the expected structure of the dataset. \n",
        "When working with the actual data, you should replace these insights with specific findings from your analysis.\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
